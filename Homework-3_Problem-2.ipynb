{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9263621d-c1f1-40dd-a004-8d6cd9cf7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries for numpy, plot and pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4fbbac66-7e0b-465d-8728-709cf815424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing breast cancer dataset\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "breast = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "148b750d-b8c7-4506-a71f-429406030cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the input variables into a matrix\n",
    "\n",
    "breast_data = breast.data\n",
    "breast_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "be37cdb8-3a37-42dd-82c1-206857e933b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27  \\\n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting the input variables matrix into a 2D panda array\n",
    "\n",
    "breast_input = pd.DataFrame(breast_data)\n",
    "breast_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4347e482-367e-4306-81ec-c455b36d81f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the outputs into a matrix\n",
    "\n",
    "breast_labels = breast.target\n",
    "breast_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8623a405-aee7-465a-85b9-c101489d0a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9c103d0f-2250-40cc-bc94-5d2d139f9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the output row matrix into a column\n",
    "\n",
    "labels = np.reshape(breast_labels,(569,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d2b1e24f-0461-45f5-a7ba-73a281de48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating the input variables and outputs into the same 2D array to create the final dataset\n",
    "\n",
    "final_breast_data = np.concatenate([breast_data,labels],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3832ce30-6839-46df-87f8-f4b5b34b6c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_breast_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a0f330ba-ea51-439c-aff7-a38778c56430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the feature names for the input variables\n",
    "\n",
    "breast_dataset = pd.DataFrame(final_breast_data)\n",
    "features = breast.feature_names\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a07bbb11-91ed-48a3-b707-7fd4f618eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_labels = np.append(features,'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f0bc3257-3816-45e4-b2a7-7006cdd01620",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_dataset.columns = features_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b4118080-ddc2-4422-b005-eae1df1551c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  label  \n",
       "0                  0.2654          0.4601                  0.11890    0.0  \n",
       "1                  0.1860          0.2750                  0.08902    0.0  \n",
       "2                  0.2430          0.3613                  0.08758    0.0  \n",
       "3                  0.2575          0.6638                  0.17300    0.0  \n",
       "4                  0.1625          0.2364                  0.07678    0.0  \n",
       "..                    ...             ...                      ...    ...  \n",
       "564                0.2216          0.2060                  0.07115    0.0  \n",
       "565                0.1628          0.2572                  0.06637    0.0  \n",
       "566                0.1418          0.2218                  0.07820    0.0  \n",
       "567                0.2650          0.4087                  0.12400    0.0  \n",
       "568                0.0000          0.2871                  0.07039    1.0  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0cc7c2dc-bd69-4035-827d-b10e60026d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_dataset['label'].replace(0, 'Benign',inplace=True)\n",
    "breast_dataset['label'].replace(1, 'Malignant',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b5ebc29c-aff8-49c5-a22a-5d418354de2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>Malignant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension      label  \n",
       "0                  0.2654          0.4601                  0.11890     Benign  \n",
       "1                  0.1860          0.2750                  0.08902     Benign  \n",
       "2                  0.2430          0.3613                  0.08758     Benign  \n",
       "3                  0.2575          0.6638                  0.17300     Benign  \n",
       "4                  0.1625          0.2364                  0.07678     Benign  \n",
       "..                    ...             ...                      ...        ...  \n",
       "564                0.2216          0.2060                  0.07115     Benign  \n",
       "565                0.1628          0.2572                  0.06637     Benign  \n",
       "566                0.1418          0.2218                  0.07820     Benign  \n",
       "567                0.2650          0.4087                  0.12400     Benign  \n",
       "568                0.0000          0.2871                  0.07039  Malignant  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9ef4afa4-822c-4e63-8809-5ba593c907bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing standard scalar and standarding the input values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separating out the features\n",
    "x = breast_dataset.loc[:, features].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "501de8af-4c8d-4eab-80b8-58a08ab4ed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.192837</td>\n",
       "      <td>1.948583</td>\n",
       "      <td>-1.123166</td>\n",
       "      <td>3.633731</td>\n",
       "      <td>-1.195110</td>\n",
       "      <td>1.411425</td>\n",
       "      <td>2.159370</td>\n",
       "      <td>-0.398395</td>\n",
       "      <td>-0.157130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.387802</td>\n",
       "      <td>-3.768172</td>\n",
       "      <td>-0.529293</td>\n",
       "      <td>1.118264</td>\n",
       "      <td>0.621775</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>0.013358</td>\n",
       "      <td>0.240991</td>\n",
       "      <td>-0.711902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.733896</td>\n",
       "      <td>-1.075174</td>\n",
       "      <td>-0.551748</td>\n",
       "      <td>0.912083</td>\n",
       "      <td>-0.177086</td>\n",
       "      <td>0.541452</td>\n",
       "      <td>-0.668166</td>\n",
       "      <td>0.097357</td>\n",
       "      <td>0.024066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.122953</td>\n",
       "      <td>10.275589</td>\n",
       "      <td>-3.232790</td>\n",
       "      <td>0.152547</td>\n",
       "      <td>-2.960878</td>\n",
       "      <td>3.053422</td>\n",
       "      <td>1.429911</td>\n",
       "      <td>1.059505</td>\n",
       "      <td>-1.405420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.935302</td>\n",
       "      <td>-1.948072</td>\n",
       "      <td>1.389767</td>\n",
       "      <td>2.940639</td>\n",
       "      <td>0.546747</td>\n",
       "      <td>-1.226495</td>\n",
       "      <td>-0.936212</td>\n",
       "      <td>0.636368</td>\n",
       "      <td>-0.263798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>6.439315</td>\n",
       "      <td>-3.576817</td>\n",
       "      <td>2.459487</td>\n",
       "      <td>1.177314</td>\n",
       "      <td>-0.074824</td>\n",
       "      <td>-2.375193</td>\n",
       "      <td>-0.596129</td>\n",
       "      <td>-0.035499</td>\n",
       "      <td>0.987945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3.793382</td>\n",
       "      <td>-3.584048</td>\n",
       "      <td>2.088476</td>\n",
       "      <td>-2.506028</td>\n",
       "      <td>-0.510723</td>\n",
       "      <td>-0.246710</td>\n",
       "      <td>-0.716326</td>\n",
       "      <td>-1.113370</td>\n",
       "      <td>-0.105201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1.256179</td>\n",
       "      <td>-1.902297</td>\n",
       "      <td>0.562731</td>\n",
       "      <td>-2.089227</td>\n",
       "      <td>1.809991</td>\n",
       "      <td>-0.534447</td>\n",
       "      <td>-0.192758</td>\n",
       "      <td>0.341887</td>\n",
       "      <td>0.393918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>10.374794</td>\n",
       "      <td>1.672010</td>\n",
       "      <td>-1.877029</td>\n",
       "      <td>-2.356031</td>\n",
       "      <td>-0.033742</td>\n",
       "      <td>0.567937</td>\n",
       "      <td>0.223082</td>\n",
       "      <td>-0.280246</td>\n",
       "      <td>-0.542035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-5.475243</td>\n",
       "      <td>-0.670637</td>\n",
       "      <td>1.490443</td>\n",
       "      <td>-2.299157</td>\n",
       "      <td>-0.184703</td>\n",
       "      <td>1.617837</td>\n",
       "      <td>1.698951</td>\n",
       "      <td>1.046330</td>\n",
       "      <td>0.374106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2         3         4         5         6  \\\n",
       "0     9.192837   1.948583 -1.123166  3.633731 -1.195110  1.411425  2.159370   \n",
       "1     2.387802  -3.768172 -0.529293  1.118264  0.621775  0.028656  0.013358   \n",
       "2     5.733896  -1.075174 -0.551748  0.912083 -0.177086  0.541452 -0.668166   \n",
       "3     7.122953  10.275589 -3.232790  0.152547 -2.960878  3.053422  1.429911   \n",
       "4     3.935302  -1.948072  1.389767  2.940639  0.546747 -1.226495 -0.936212   \n",
       "..         ...        ...       ...       ...       ...       ...       ...   \n",
       "564   6.439315  -3.576817  2.459487  1.177314 -0.074824 -2.375193 -0.596129   \n",
       "565   3.793382  -3.584048  2.088476 -2.506028 -0.510723 -0.246710 -0.716326   \n",
       "566   1.256179  -1.902297  0.562731 -2.089227  1.809991 -0.534447 -0.192758   \n",
       "567  10.374794   1.672010 -1.877029 -2.356031 -0.033742  0.567937  0.223082   \n",
       "568  -5.475243  -0.670637  1.490443 -2.299157 -0.184703  1.617837  1.698951   \n",
       "\n",
       "            7         8  \n",
       "0   -0.398395 -0.157130  \n",
       "1    0.240991 -0.711902  \n",
       "2    0.097357  0.024066  \n",
       "3    1.059505 -1.405420  \n",
       "4    0.636368 -0.263798  \n",
       "..        ...       ...  \n",
       "564 -0.035499  0.987945  \n",
       "565 -1.113370 -0.105201  \n",
       "566  0.341887  0.393918  \n",
       "567 -0.280246 -0.542035  \n",
       "568  1.046330  0.374106  \n",
       "\n",
       "[569 rows x 9 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing Principal Component Analysis  (Number of principal components, K=2)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=9)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "97c48de1-b7f4-4dd6-b53b-1aeaa44ed0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the principal components as the input variables\n",
    "\n",
    "X = principalComponents.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8]].values\n",
    "Y = breast_dataset.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ce7c132a-d43b-4006-a689-c111b2ed1911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.19283683,  1.94858307, -1.12316616, ...,  2.15936964,\n",
       "        -0.39839517, -0.15712985],\n",
       "       [ 2.3878018 , -3.76817174, -0.52929269, ...,  0.01335788,\n",
       "         0.2409914 , -0.71190203],\n",
       "       [ 5.73389628, -1.0751738 , -0.5517476 , ..., -0.6681664 ,\n",
       "         0.09735689,  0.02406581],\n",
       "       ...,\n",
       "       [ 1.25617928, -1.90229671,  0.56273052, ..., -0.19275818,\n",
       "         0.34188748,  0.39391846],\n",
       "       [10.37479406,  1.67201011, -1.87702933, ...,  0.22308156,\n",
       "        -0.2802463 , -0.54203507],\n",
       "       [-5.4752433 , -0.67063679,  1.49044307, ...,  1.69895109,\n",
       "         1.04633007,  0.37410599]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c1a4829f-7dcc-47bb-8700-3d371ecf2ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Benign', 'Benign', 'Benign', 'Benign', 'Benign', 'Benign',\n",
       "       'Benign', 'Benign', 'Benign', 'Benign', 'Benign', 'Benign',\n",
       "       'Benign', 'Benign', 'Benign', 'Benign', 'Benign', 'Benign',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Benign', 'Benign', 'Benign', 'Benign', 'Benign', 'Benign',\n",
       "       'Benign', 'Benign', 'Benign', 'Benign', 'Benign', 'Benign',\n",
       "       'Benign', 'Benign', 'Malignant', 'Benign', 'Benign', 'Benign',\n",
       "       'Benign', 'Benign', 'Benign', 'Benign', 'Benign', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Benign', 'Malignant', 'Benign', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Benign', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Malignant', 'Benign',\n",
       "       'Benign', 'Malignant', 'Benign', 'Malignant', 'Benign', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Benign', 'Benign',\n",
       "       'Malignant', 'Benign', 'Benign', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Benign', 'Benign',\n",
       "       'Benign', 'Malignant', 'Benign', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Benign', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Benign', 'Benign', 'Malignant', 'Benign',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Benign', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Benign', 'Benign', 'Malignant', 'Benign', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Benign', 'Malignant',\n",
       "       'Benign', 'Benign', 'Benign', 'Benign', 'Malignant', 'Benign',\n",
       "       'Benign', 'Benign', 'Malignant', 'Benign', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Malignant', 'Benign',\n",
       "       'Benign', 'Benign', 'Benign', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Benign', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Benign', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Benign', 'Benign', 'Benign',\n",
       "       'Benign', 'Benign', 'Benign', 'Benign', 'Benign', 'Benign',\n",
       "       'Benign', 'Benign', 'Benign', 'Benign', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Benign', 'Malignant',\n",
       "       'Benign', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Benign', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Benign', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Benign', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Benign', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Benign', 'Benign',\n",
       "       'Malignant', 'Benign', 'Benign', 'Benign', 'Malignant', 'Benign',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Malignant', 'Benign',\n",
       "       'Benign', 'Malignant', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Benign', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Benign', 'Malignant', 'Benign', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Malignant', 'Benign', 'Malignant',\n",
       "       'Benign', 'Malignant', 'Benign', 'Benign', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Benign',\n",
       "       'Malignant', 'Benign', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Malignant', 'Malignant', 'Benign', 'Benign',\n",
       "       'Benign', 'Benign', 'Benign', 'Benign', 'Malignant'], dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2567ce09-41e9-4c2a-b875-b498c53ab55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing train_test_split\n",
    "#splitting the dataset into 80% and 20% split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fd37aa55-f8ee-424e-96ce-7a8c33c83fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fe04be56-5d33-4591-9ddd-2be5928cc7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Benign', 'Malignant', 'Benign', 'Malignant', 'Malignant',\n",
       "       'Malignant', 'Benign', 'Benign', 'Malignant'], dtype=object)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the classifier on the test data\n",
    "\n",
    "Y_pred = classifier.predict(X_test)\n",
    "Y_pred[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "09c55652-260d-449c-9cad-2266749f27d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46,  3],\n",
       "       [ 0, 65]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing and creating confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c5c5654d-86f1-4869-9670-18633ff1c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n",
      "Precision: 0.9558823529411765\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#importing and printing accuracy, precision and recall of the classifier\n",
    "\n",
    "from sklearn import metrics\n",
    "pos_label=['Benign', 'Malignant']\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred, pos_label= \"Malignant\"))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred, pos_label= \"Malignant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7b0833b6-b784-4a1d-9a48-0dca47c13f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 427.9555555555555, 'Predicted label')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the confusion matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "class_names=['Benign','Malignant'] # name of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"BuPu\" ,fmt='g', xticklabels=class_names,yticklabels=class_names, )\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f7e252d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.192837</td>\n",
       "      <td>1.948583</td>\n",
       "      <td>-1.123166</td>\n",
       "      <td>3.633731</td>\n",
       "      <td>-1.195110</td>\n",
       "      <td>1.411424</td>\n",
       "      <td>2.159370</td>\n",
       "      <td>-0.398407</td>\n",
       "      <td>-0.157118</td>\n",
       "      <td>-0.877402</td>\n",
       "      <td>0.262955</td>\n",
       "      <td>-0.859014</td>\n",
       "      <td>0.103388</td>\n",
       "      <td>-0.690804</td>\n",
       "      <td>-0.601793</td>\n",
       "      <td>0.745116</td>\n",
       "      <td>-0.265471</td>\n",
       "      <td>-0.549563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.387802</td>\n",
       "      <td>-3.768172</td>\n",
       "      <td>-0.529293</td>\n",
       "      <td>1.118264</td>\n",
       "      <td>0.621775</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>0.013358</td>\n",
       "      <td>0.240988</td>\n",
       "      <td>-0.711905</td>\n",
       "      <td>1.106995</td>\n",
       "      <td>0.813120</td>\n",
       "      <td>0.157923</td>\n",
       "      <td>-0.943529</td>\n",
       "      <td>-0.653475</td>\n",
       "      <td>0.008975</td>\n",
       "      <td>-0.648809</td>\n",
       "      <td>-0.017212</td>\n",
       "      <td>0.318297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.733896</td>\n",
       "      <td>-1.075174</td>\n",
       "      <td>-0.551748</td>\n",
       "      <td>0.912083</td>\n",
       "      <td>-0.177086</td>\n",
       "      <td>0.541452</td>\n",
       "      <td>-0.668166</td>\n",
       "      <td>0.097374</td>\n",
       "      <td>0.024066</td>\n",
       "      <td>0.454275</td>\n",
       "      <td>-0.605604</td>\n",
       "      <td>0.124387</td>\n",
       "      <td>-0.410627</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>0.483420</td>\n",
       "      <td>0.325111</td>\n",
       "      <td>0.190918</td>\n",
       "      <td>-0.087975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.122953</td>\n",
       "      <td>10.275589</td>\n",
       "      <td>-3.232790</td>\n",
       "      <td>0.152547</td>\n",
       "      <td>-2.960878</td>\n",
       "      <td>3.053422</td>\n",
       "      <td>1.429911</td>\n",
       "      <td>1.059565</td>\n",
       "      <td>-1.405440</td>\n",
       "      <td>-1.116975</td>\n",
       "      <td>-1.151514</td>\n",
       "      <td>1.011316</td>\n",
       "      <td>-0.933271</td>\n",
       "      <td>-0.487417</td>\n",
       "      <td>-0.168848</td>\n",
       "      <td>0.051370</td>\n",
       "      <td>0.482634</td>\n",
       "      <td>-0.035875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.935302</td>\n",
       "      <td>-1.948072</td>\n",
       "      <td>1.389767</td>\n",
       "      <td>2.940639</td>\n",
       "      <td>0.546747</td>\n",
       "      <td>-1.226495</td>\n",
       "      <td>-0.936213</td>\n",
       "      <td>0.636376</td>\n",
       "      <td>-0.263805</td>\n",
       "      <td>0.377704</td>\n",
       "      <td>0.651360</td>\n",
       "      <td>-0.110515</td>\n",
       "      <td>0.387948</td>\n",
       "      <td>-0.539181</td>\n",
       "      <td>0.310319</td>\n",
       "      <td>-0.152606</td>\n",
       "      <td>0.133142</td>\n",
       "      <td>-0.018714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>6.439315</td>\n",
       "      <td>-3.576817</td>\n",
       "      <td>2.459487</td>\n",
       "      <td>1.177314</td>\n",
       "      <td>-0.074824</td>\n",
       "      <td>-2.375193</td>\n",
       "      <td>-0.596130</td>\n",
       "      <td>-0.035471</td>\n",
       "      <td>0.987929</td>\n",
       "      <td>0.256989</td>\n",
       "      <td>-0.062651</td>\n",
       "      <td>0.123342</td>\n",
       "      <td>-0.051723</td>\n",
       "      <td>-0.404290</td>\n",
       "      <td>0.652750</td>\n",
       "      <td>0.147642</td>\n",
       "      <td>0.348266</td>\n",
       "      <td>-0.195214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3.793382</td>\n",
       "      <td>-3.584048</td>\n",
       "      <td>2.088476</td>\n",
       "      <td>-2.506028</td>\n",
       "      <td>-0.510723</td>\n",
       "      <td>-0.246710</td>\n",
       "      <td>-0.716326</td>\n",
       "      <td>-1.113360</td>\n",
       "      <td>-0.105207</td>\n",
       "      <td>-0.108632</td>\n",
       "      <td>0.244804</td>\n",
       "      <td>0.222753</td>\n",
       "      <td>-0.192637</td>\n",
       "      <td>0.015555</td>\n",
       "      <td>-0.069975</td>\n",
       "      <td>-0.138184</td>\n",
       "      <td>0.293495</td>\n",
       "      <td>-0.109046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1.256179</td>\n",
       "      <td>-1.902297</td>\n",
       "      <td>0.562731</td>\n",
       "      <td>-2.089227</td>\n",
       "      <td>1.809991</td>\n",
       "      <td>-0.534447</td>\n",
       "      <td>-0.192758</td>\n",
       "      <td>0.341887</td>\n",
       "      <td>0.393917</td>\n",
       "      <td>0.520877</td>\n",
       "      <td>-0.840512</td>\n",
       "      <td>0.096473</td>\n",
       "      <td>0.157418</td>\n",
       "      <td>0.285691</td>\n",
       "      <td>0.090998</td>\n",
       "      <td>-0.232648</td>\n",
       "      <td>-0.065615</td>\n",
       "      <td>0.021108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>10.374794</td>\n",
       "      <td>1.672010</td>\n",
       "      <td>-1.877029</td>\n",
       "      <td>-2.356031</td>\n",
       "      <td>-0.033742</td>\n",
       "      <td>0.567936</td>\n",
       "      <td>0.223082</td>\n",
       "      <td>-0.280239</td>\n",
       "      <td>-0.542035</td>\n",
       "      <td>-0.089296</td>\n",
       "      <td>-0.178628</td>\n",
       "      <td>-0.697461</td>\n",
       "      <td>1.225195</td>\n",
       "      <td>0.218698</td>\n",
       "      <td>0.229591</td>\n",
       "      <td>-0.061047</td>\n",
       "      <td>-0.168514</td>\n",
       "      <td>-0.306874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-5.475243</td>\n",
       "      <td>-0.670637</td>\n",
       "      <td>1.490443</td>\n",
       "      <td>-2.299157</td>\n",
       "      <td>-0.184703</td>\n",
       "      <td>1.617837</td>\n",
       "      <td>1.698952</td>\n",
       "      <td>1.046354</td>\n",
       "      <td>0.374101</td>\n",
       "      <td>-0.047726</td>\n",
       "      <td>-0.144094</td>\n",
       "      <td>-0.179496</td>\n",
       "      <td>0.678897</td>\n",
       "      <td>-1.170725</td>\n",
       "      <td>0.217343</td>\n",
       "      <td>0.921288</td>\n",
       "      <td>-0.304810</td>\n",
       "      <td>-0.180972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1         2         3         4         5         6   \\\n",
       "0     9.192837   1.948583 -1.123166  3.633731 -1.195110  1.411424  2.159370   \n",
       "1     2.387802  -3.768172 -0.529293  1.118264  0.621775  0.028656  0.013358   \n",
       "2     5.733896  -1.075174 -0.551748  0.912083 -0.177086  0.541452 -0.668166   \n",
       "3     7.122953  10.275589 -3.232790  0.152547 -2.960878  3.053422  1.429911   \n",
       "4     3.935302  -1.948072  1.389767  2.940639  0.546747 -1.226495 -0.936213   \n",
       "..         ...        ...       ...       ...       ...       ...       ...   \n",
       "564   6.439315  -3.576817  2.459487  1.177314 -0.074824 -2.375193 -0.596130   \n",
       "565   3.793382  -3.584048  2.088476 -2.506028 -0.510723 -0.246710 -0.716326   \n",
       "566   1.256179  -1.902297  0.562731 -2.089227  1.809991 -0.534447 -0.192758   \n",
       "567  10.374794   1.672010 -1.877029 -2.356031 -0.033742  0.567936  0.223082   \n",
       "568  -5.475243  -0.670637  1.490443 -2.299157 -0.184703  1.617837  1.698952   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "0   -0.398407 -0.157118 -0.877402  0.262955 -0.859014  0.103388 -0.690804   \n",
       "1    0.240988 -0.711905  1.106995  0.813120  0.157923 -0.943529 -0.653475   \n",
       "2    0.097374  0.024066  0.454275 -0.605604  0.124387 -0.410627  0.016680   \n",
       "3    1.059565 -1.405440 -1.116975 -1.151514  1.011316 -0.933271 -0.487417   \n",
       "4    0.636376 -0.263805  0.377704  0.651360 -0.110515  0.387948 -0.539181   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "564 -0.035471  0.987929  0.256989 -0.062651  0.123342 -0.051723 -0.404290   \n",
       "565 -1.113360 -0.105207 -0.108632  0.244804  0.222753 -0.192637  0.015555   \n",
       "566  0.341887  0.393917  0.520877 -0.840512  0.096473  0.157418  0.285691   \n",
       "567 -0.280239 -0.542035 -0.089296 -0.178628 -0.697461  1.225195  0.218698   \n",
       "568  1.046354  0.374101 -0.047726 -0.144094 -0.179496  0.678897 -1.170725   \n",
       "\n",
       "           14        15        16        17  \n",
       "0   -0.601793  0.745116 -0.265471 -0.549563  \n",
       "1    0.008975 -0.648809 -0.017212  0.318297  \n",
       "2    0.483420  0.325111  0.190918 -0.087975  \n",
       "3   -0.168848  0.051370  0.482634 -0.035875  \n",
       "4    0.310319 -0.152606  0.133142 -0.018714  \n",
       "..        ...       ...       ...       ...  \n",
       "564  0.652750  0.147642  0.348266 -0.195214  \n",
       "565 -0.069975 -0.138184  0.293495 -0.109046  \n",
       "566  0.090998 -0.232648 -0.065615  0.021108  \n",
       "567  0.229591 -0.061047 -0.168514 -0.306874  \n",
       "568  0.217343  0.921288 -0.304810 -0.180972  \n",
       "\n",
       "[569 rows x 18 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing Principal Component Analysis  (Number of principal components, K=18)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=18)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bcc7a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the principal components as the input variables\n",
    "\n",
    "X = principalComponents.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8,9,10,11,12,13,14,15,16,17]].values\n",
    "Y = breast_dataset.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "84915412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing train_test_split\n",
    "#splitting the dataset into 80% and 20% split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e3608479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;background-color: white;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fa1c8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the classifier on the test data\n",
    "\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ec77d6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n",
      "Precision: 0.9558823529411765\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#importing and printing accuracy, precision and recall of the classifier\n",
    "\n",
    "from sklearn import metrics\n",
    "pos_label=['Benign', 'Malignant']\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred, pos_label= \"Malignant\"))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred, pos_label= \"Malignant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4d0587cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.192837</td>\n",
       "      <td>1.948583</td>\n",
       "      <td>-1.123167</td>\n",
       "      <td>3.633739</td>\n",
       "      <td>-1.195163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.387802</td>\n",
       "      <td>-3.768172</td>\n",
       "      <td>-0.529292</td>\n",
       "      <td>1.118259</td>\n",
       "      <td>0.621806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.733896</td>\n",
       "      <td>-1.075174</td>\n",
       "      <td>-0.551748</td>\n",
       "      <td>0.912084</td>\n",
       "      <td>-0.177097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.122953</td>\n",
       "      <td>10.275589</td>\n",
       "      <td>-3.232789</td>\n",
       "      <td>0.152547</td>\n",
       "      <td>-2.960881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.935302</td>\n",
       "      <td>-1.948072</td>\n",
       "      <td>1.389767</td>\n",
       "      <td>2.940639</td>\n",
       "      <td>0.546751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>6.439315</td>\n",
       "      <td>-3.576817</td>\n",
       "      <td>2.459486</td>\n",
       "      <td>1.177315</td>\n",
       "      <td>-0.074831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3.793382</td>\n",
       "      <td>-3.584048</td>\n",
       "      <td>2.088477</td>\n",
       "      <td>-2.506029</td>\n",
       "      <td>-0.510714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1.256179</td>\n",
       "      <td>-1.902297</td>\n",
       "      <td>0.562731</td>\n",
       "      <td>-2.089229</td>\n",
       "      <td>1.810005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>10.374794</td>\n",
       "      <td>1.672010</td>\n",
       "      <td>-1.877029</td>\n",
       "      <td>-2.356030</td>\n",
       "      <td>-0.033744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-5.475243</td>\n",
       "      <td>-0.670637</td>\n",
       "      <td>1.490442</td>\n",
       "      <td>-2.299149</td>\n",
       "      <td>-0.184760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2         3         4\n",
       "0     9.192837   1.948583 -1.123167  3.633739 -1.195163\n",
       "1     2.387802  -3.768172 -0.529292  1.118259  0.621806\n",
       "2     5.733896  -1.075174 -0.551748  0.912084 -0.177097\n",
       "3     7.122953  10.275589 -3.232789  0.152547 -2.960881\n",
       "4     3.935302  -1.948072  1.389767  2.940639  0.546751\n",
       "..         ...        ...       ...       ...       ...\n",
       "564   6.439315  -3.576817  2.459486  1.177315 -0.074831\n",
       "565   3.793382  -3.584048  2.088477 -2.506029 -0.510714\n",
       "566   1.256179  -1.902297  0.562731 -2.089229  1.810005\n",
       "567  10.374794   1.672010 -1.877029 -2.356030 -0.033744\n",
       "568  -5.475243  -0.670637  1.490442 -2.299149 -0.184760\n",
       "\n",
       "[569 rows x 5 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing Principal Component Analysis  (Number of principal components, K=5)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c6cac5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the principal components as the input variables\n",
    "\n",
    "X = principalComponents.iloc[:, [0, 1, 2, 3, 4]].values\n",
    "Y = breast_dataset.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "021c3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing train_test_split\n",
    "#splitting the dataset into 80% and 20% split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bd004901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "18b46ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the classifier on the test data\n",
    "\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9db3d109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n",
      "Precision: 0.9552238805970149\n",
      "Recall: 0.9846153846153847\n"
     ]
    }
   ],
   "source": [
    "#importing and printing accuracy, precision and recall of the classifier\n",
    "\n",
    "from sklearn import metrics\n",
    "pos_label=['Benign', 'Malignant']\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred, pos_label= \"Malignant\"))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred, pos_label= \"Malignant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ebe71003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.192837</td>\n",
       "      <td>1.948583</td>\n",
       "      <td>-1.123166</td>\n",
       "      <td>3.633730</td>\n",
       "      <td>-1.195109</td>\n",
       "      <td>1.411424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.387802</td>\n",
       "      <td>-3.768172</td>\n",
       "      <td>-0.529293</td>\n",
       "      <td>1.118264</td>\n",
       "      <td>0.621775</td>\n",
       "      <td>0.028657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.733896</td>\n",
       "      <td>-1.075174</td>\n",
       "      <td>-0.551747</td>\n",
       "      <td>0.912083</td>\n",
       "      <td>-0.177083</td>\n",
       "      <td>0.541457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.122953</td>\n",
       "      <td>10.275589</td>\n",
       "      <td>-3.232790</td>\n",
       "      <td>0.152547</td>\n",
       "      <td>-2.960878</td>\n",
       "      <td>3.053419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.935302</td>\n",
       "      <td>-1.948072</td>\n",
       "      <td>1.389767</td>\n",
       "      <td>2.940639</td>\n",
       "      <td>0.546746</td>\n",
       "      <td>-1.226498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>6.439315</td>\n",
       "      <td>-3.576817</td>\n",
       "      <td>2.459487</td>\n",
       "      <td>1.177314</td>\n",
       "      <td>-0.074821</td>\n",
       "      <td>-2.375190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3.793382</td>\n",
       "      <td>-3.584048</td>\n",
       "      <td>2.088476</td>\n",
       "      <td>-2.506028</td>\n",
       "      <td>-0.510722</td>\n",
       "      <td>-0.246711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1.256179</td>\n",
       "      <td>-1.902297</td>\n",
       "      <td>0.562731</td>\n",
       "      <td>-2.089227</td>\n",
       "      <td>1.809992</td>\n",
       "      <td>-0.534446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>10.374794</td>\n",
       "      <td>1.672010</td>\n",
       "      <td>-1.877029</td>\n",
       "      <td>-2.356032</td>\n",
       "      <td>-0.033742</td>\n",
       "      <td>0.567936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-5.475243</td>\n",
       "      <td>-0.670637</td>\n",
       "      <td>1.490443</td>\n",
       "      <td>-2.299158</td>\n",
       "      <td>-0.184702</td>\n",
       "      <td>1.617840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2         3         4         5\n",
       "0     9.192837   1.948583 -1.123166  3.633730 -1.195109  1.411424\n",
       "1     2.387802  -3.768172 -0.529293  1.118264  0.621775  0.028657\n",
       "2     5.733896  -1.075174 -0.551747  0.912083 -0.177083  0.541457\n",
       "3     7.122953  10.275589 -3.232790  0.152547 -2.960878  3.053419\n",
       "4     3.935302  -1.948072  1.389767  2.940639  0.546746 -1.226498\n",
       "..         ...        ...       ...       ...       ...       ...\n",
       "564   6.439315  -3.576817  2.459487  1.177314 -0.074821 -2.375190\n",
       "565   3.793382  -3.584048  2.088476 -2.506028 -0.510722 -0.246711\n",
       "566   1.256179  -1.902297  0.562731 -2.089227  1.809992 -0.534446\n",
       "567  10.374794   1.672010 -1.877029 -2.356032 -0.033742  0.567936\n",
       "568  -5.475243  -0.670637  1.490443 -2.299158 -0.184702  1.617840\n",
       "\n",
       "[569 rows x 6 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing Principal Component Analysis  (Number of principal components, K=6)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f095a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the principal components as the input variables\n",
    "\n",
    "X = principalComponents.iloc[:, [0, 1, 2, 3, 4,5]].values\n",
    "Y = breast_dataset.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3f0a4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing train_test_split\n",
    "#splitting the dataset into 80% and 20% split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f4b34212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;background-color: white;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "56e58f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the classifier on the test data\n",
    "\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0e5e1d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n",
      "Precision: 0.9545454545454546\n",
      "Recall: 0.9692307692307692\n"
     ]
    }
   ],
   "source": [
    "#importing and printing accuracy, precision and recall of the classifier\n",
    "\n",
    "from sklearn import metrics\n",
    "pos_label=['Benign', 'Malignant']\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred, pos_label= \"Malignant\"))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred, pos_label= \"Malignant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e7b156d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.192837</td>\n",
       "      <td>1.948583</td>\n",
       "      <td>-1.123166</td>\n",
       "      <td>3.633731</td>\n",
       "      <td>-1.195110</td>\n",
       "      <td>1.411428</td>\n",
       "      <td>2.159376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.387802</td>\n",
       "      <td>-3.768172</td>\n",
       "      <td>-0.529293</td>\n",
       "      <td>1.118264</td>\n",
       "      <td>0.621775</td>\n",
       "      <td>0.028654</td>\n",
       "      <td>0.013354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.733896</td>\n",
       "      <td>-1.075174</td>\n",
       "      <td>-0.551748</td>\n",
       "      <td>0.912083</td>\n",
       "      <td>-0.177086</td>\n",
       "      <td>0.541453</td>\n",
       "      <td>-0.668165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.122953</td>\n",
       "      <td>10.275589</td>\n",
       "      <td>-3.232790</td>\n",
       "      <td>0.152547</td>\n",
       "      <td>-2.960879</td>\n",
       "      <td>3.053421</td>\n",
       "      <td>1.429897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.935302</td>\n",
       "      <td>-1.948072</td>\n",
       "      <td>1.389767</td>\n",
       "      <td>2.940639</td>\n",
       "      <td>0.546747</td>\n",
       "      <td>-1.226495</td>\n",
       "      <td>-0.936217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>6.439315</td>\n",
       "      <td>-3.576817</td>\n",
       "      <td>2.459487</td>\n",
       "      <td>1.177314</td>\n",
       "      <td>-0.074824</td>\n",
       "      <td>-2.375193</td>\n",
       "      <td>-0.596147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3.793382</td>\n",
       "      <td>-3.584048</td>\n",
       "      <td>2.088476</td>\n",
       "      <td>-2.506028</td>\n",
       "      <td>-0.510723</td>\n",
       "      <td>-0.246710</td>\n",
       "      <td>-0.716336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1.256179</td>\n",
       "      <td>-1.902297</td>\n",
       "      <td>0.562731</td>\n",
       "      <td>-2.089227</td>\n",
       "      <td>1.809991</td>\n",
       "      <td>-0.534447</td>\n",
       "      <td>-0.192764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>10.374794</td>\n",
       "      <td>1.672010</td>\n",
       "      <td>-1.877029</td>\n",
       "      <td>-2.356031</td>\n",
       "      <td>-0.033742</td>\n",
       "      <td>0.567938</td>\n",
       "      <td>0.223087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-5.475243</td>\n",
       "      <td>-0.670637</td>\n",
       "      <td>1.490443</td>\n",
       "      <td>-2.299157</td>\n",
       "      <td>-0.184703</td>\n",
       "      <td>1.617839</td>\n",
       "      <td>1.698951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2         3         4         5         6\n",
       "0     9.192837   1.948583 -1.123166  3.633731 -1.195110  1.411428  2.159376\n",
       "1     2.387802  -3.768172 -0.529293  1.118264  0.621775  0.028654  0.013354\n",
       "2     5.733896  -1.075174 -0.551748  0.912083 -0.177086  0.541453 -0.668165\n",
       "3     7.122953  10.275589 -3.232790  0.152547 -2.960879  3.053421  1.429897\n",
       "4     3.935302  -1.948072  1.389767  2.940639  0.546747 -1.226495 -0.936217\n",
       "..         ...        ...       ...       ...       ...       ...       ...\n",
       "564   6.439315  -3.576817  2.459487  1.177314 -0.074824 -2.375193 -0.596147\n",
       "565   3.793382  -3.584048  2.088476 -2.506028 -0.510723 -0.246710 -0.716336\n",
       "566   1.256179  -1.902297  0.562731 -2.089227  1.809991 -0.534447 -0.192764\n",
       "567  10.374794   1.672010 -1.877029 -2.356031 -0.033742  0.567938  0.223087\n",
       "568  -5.475243  -0.670637  1.490443 -2.299157 -0.184703  1.617839  1.698951\n",
       "\n",
       "[569 rows x 7 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing Principal Component Analysis  (Number of principal components, K=6)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=7)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0f0d3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the principal components as the input variables\n",
    "\n",
    "X = principalComponents.iloc[:, [0,1, 2, 3, 4,5,6]].values\n",
    "Y = breast_dataset.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b6adb1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing train_test_split\n",
    "#splitting the dataset into 80% and 20% split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "971a3363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;background-color: white;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4b3a9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the classifier on the test data\n",
    "\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b576174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n",
      "Precision: 0.9552238805970149\n",
      "Recall: 0.9846153846153847\n"
     ]
    }
   ],
   "source": [
    "#importing and printing accuracy, precision and recall of the classifier\n",
    "\n",
    "from sklearn import metrics\n",
    "pos_label=['Benign', 'Malignant']\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred, pos_label= \"Malignant\"))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred, pos_label= \"Malignant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c81e5710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.192837</td>\n",
       "      <td>1.948583</td>\n",
       "      <td>-1.123166</td>\n",
       "      <td>3.633731</td>\n",
       "      <td>-1.195110</td>\n",
       "      <td>1.411426</td>\n",
       "      <td>2.159364</td>\n",
       "      <td>-0.398422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.387802</td>\n",
       "      <td>-3.768172</td>\n",
       "      <td>-0.529293</td>\n",
       "      <td>1.118264</td>\n",
       "      <td>0.621775</td>\n",
       "      <td>0.028657</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>0.240986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.733896</td>\n",
       "      <td>-1.075174</td>\n",
       "      <td>-0.551748</td>\n",
       "      <td>0.912083</td>\n",
       "      <td>-0.177086</td>\n",
       "      <td>0.541449</td>\n",
       "      <td>-0.668148</td>\n",
       "      <td>0.097417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.122953</td>\n",
       "      <td>10.275589</td>\n",
       "      <td>-3.232790</td>\n",
       "      <td>0.152547</td>\n",
       "      <td>-2.960878</td>\n",
       "      <td>3.053419</td>\n",
       "      <td>1.429927</td>\n",
       "      <td>1.059607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.935302</td>\n",
       "      <td>-1.948072</td>\n",
       "      <td>1.389767</td>\n",
       "      <td>2.940639</td>\n",
       "      <td>0.546747</td>\n",
       "      <td>-1.226495</td>\n",
       "      <td>-0.936210</td>\n",
       "      <td>0.636381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>6.439315</td>\n",
       "      <td>-3.576817</td>\n",
       "      <td>2.459486</td>\n",
       "      <td>1.177314</td>\n",
       "      <td>-0.074824</td>\n",
       "      <td>-2.375198</td>\n",
       "      <td>-0.596102</td>\n",
       "      <td>-0.035410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3.793382</td>\n",
       "      <td>-3.584048</td>\n",
       "      <td>2.088476</td>\n",
       "      <td>-2.506028</td>\n",
       "      <td>-0.510722</td>\n",
       "      <td>-0.246714</td>\n",
       "      <td>-0.716308</td>\n",
       "      <td>-1.113319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1.256179</td>\n",
       "      <td>-1.902297</td>\n",
       "      <td>0.562731</td>\n",
       "      <td>-2.089227</td>\n",
       "      <td>1.809991</td>\n",
       "      <td>-0.534447</td>\n",
       "      <td>-0.192758</td>\n",
       "      <td>0.341887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>10.374794</td>\n",
       "      <td>1.672010</td>\n",
       "      <td>-1.877029</td>\n",
       "      <td>-2.356031</td>\n",
       "      <td>-0.033742</td>\n",
       "      <td>0.567937</td>\n",
       "      <td>0.223081</td>\n",
       "      <td>-0.280238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-5.475243</td>\n",
       "      <td>-0.670637</td>\n",
       "      <td>1.490443</td>\n",
       "      <td>-2.299157</td>\n",
       "      <td>-0.184703</td>\n",
       "      <td>1.617841</td>\n",
       "      <td>1.698935</td>\n",
       "      <td>1.046322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2         3         4         5         6  \\\n",
       "0     9.192837   1.948583 -1.123166  3.633731 -1.195110  1.411426  2.159364   \n",
       "1     2.387802  -3.768172 -0.529293  1.118264  0.621775  0.028657  0.013356   \n",
       "2     5.733896  -1.075174 -0.551748  0.912083 -0.177086  0.541449 -0.668148   \n",
       "3     7.122953  10.275589 -3.232790  0.152547 -2.960878  3.053419  1.429927   \n",
       "4     3.935302  -1.948072  1.389767  2.940639  0.546747 -1.226495 -0.936210   \n",
       "..         ...        ...       ...       ...       ...       ...       ...   \n",
       "564   6.439315  -3.576817  2.459486  1.177314 -0.074824 -2.375198 -0.596102   \n",
       "565   3.793382  -3.584048  2.088476 -2.506028 -0.510722 -0.246714 -0.716308   \n",
       "566   1.256179  -1.902297  0.562731 -2.089227  1.809991 -0.534447 -0.192758   \n",
       "567  10.374794   1.672010 -1.877029 -2.356031 -0.033742  0.567937  0.223081   \n",
       "568  -5.475243  -0.670637  1.490443 -2.299157 -0.184703  1.617841  1.698935   \n",
       "\n",
       "            7  \n",
       "0   -0.398422  \n",
       "1    0.240986  \n",
       "2    0.097417  \n",
       "3    1.059607  \n",
       "4    0.636381  \n",
       "..        ...  \n",
       "564 -0.035410  \n",
       "565 -1.113319  \n",
       "566  0.341887  \n",
       "567 -0.280238  \n",
       "568  1.046322  \n",
       "\n",
       "[569 rows x 8 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing Principal Component Analysis  (Number of principal components, K=6)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=8)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f1ae83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the principal components as the input variables\n",
    "\n",
    "X = principalComponents.iloc[:, [0,1, 2, 3, 4,5,6,7]].values\n",
    "Y = breast_dataset.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8217eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing train_test_split\n",
    "#splitting the dataset into 80% and 20% split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "804a6eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-19 {color: black;background-color: white;}#sk-container-id-19 pre{padding: 0;}#sk-container-id-19 div.sk-toggleable {background-color: white;}#sk-container-id-19 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-19 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-19 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-19 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-19 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-19 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-19 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-19 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-19 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-19 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-19 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-19 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-19 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-19 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-19 div.sk-item {position: relative;z-index: 1;}#sk-container-id-19 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-19 div.sk-item::before, #sk-container-id-19 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-19 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-19 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-19 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-19 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-19 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-19 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-19 div.sk-label-container {text-align: center;}#sk-container-id-19 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-19 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-19\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" checked><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "88ea0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the classifier on the test data\n",
    "\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4dbb42ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n",
      "Precision: 0.9552238805970149\n",
      "Recall: 0.9846153846153847\n"
     ]
    }
   ],
   "source": [
    "#importing and printing accuracy, precision and recall of the classifier\n",
    "\n",
    "from sklearn import metrics\n",
    "pos_label=['Benign', 'Malignant']\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred, pos_label= \"Malignant\"))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred, pos_label= \"Malignant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ba7ce1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.192837</td>\n",
       "      <td>1.948583</td>\n",
       "      <td>-1.123166</td>\n",
       "      <td>3.633731</td>\n",
       "      <td>-1.195110</td>\n",
       "      <td>1.411424</td>\n",
       "      <td>2.159370</td>\n",
       "      <td>-0.398407</td>\n",
       "      <td>-0.157118</td>\n",
       "      <td>-0.877402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745116</td>\n",
       "      <td>-0.265471</td>\n",
       "      <td>-0.549563</td>\n",
       "      <td>-0.133768</td>\n",
       "      <td>0.345565</td>\n",
       "      <td>0.096515</td>\n",
       "      <td>0.068850</td>\n",
       "      <td>0.084519</td>\n",
       "      <td>-0.175256</td>\n",
       "      <td>-0.151020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.387802</td>\n",
       "      <td>-3.768172</td>\n",
       "      <td>-0.529293</td>\n",
       "      <td>1.118264</td>\n",
       "      <td>0.621775</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>0.013358</td>\n",
       "      <td>0.240988</td>\n",
       "      <td>-0.711905</td>\n",
       "      <td>1.106995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.648809</td>\n",
       "      <td>-0.017212</td>\n",
       "      <td>0.318297</td>\n",
       "      <td>0.247565</td>\n",
       "      <td>-0.114133</td>\n",
       "      <td>-0.077327</td>\n",
       "      <td>-0.094578</td>\n",
       "      <td>-0.217718</td>\n",
       "      <td>0.011290</td>\n",
       "      <td>-0.170510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.733896</td>\n",
       "      <td>-1.075174</td>\n",
       "      <td>-0.551748</td>\n",
       "      <td>0.912083</td>\n",
       "      <td>-0.177086</td>\n",
       "      <td>0.541452</td>\n",
       "      <td>-0.668166</td>\n",
       "      <td>0.097374</td>\n",
       "      <td>0.024066</td>\n",
       "      <td>0.454275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325111</td>\n",
       "      <td>0.190918</td>\n",
       "      <td>-0.087975</td>\n",
       "      <td>0.392626</td>\n",
       "      <td>-0.204532</td>\n",
       "      <td>0.311067</td>\n",
       "      <td>-0.060309</td>\n",
       "      <td>-0.074291</td>\n",
       "      <td>0.102762</td>\n",
       "      <td>0.171158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.122953</td>\n",
       "      <td>10.275589</td>\n",
       "      <td>-3.232790</td>\n",
       "      <td>0.152547</td>\n",
       "      <td>-2.960878</td>\n",
       "      <td>3.053422</td>\n",
       "      <td>1.429911</td>\n",
       "      <td>1.059565</td>\n",
       "      <td>-1.405440</td>\n",
       "      <td>-1.116975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051370</td>\n",
       "      <td>0.482634</td>\n",
       "      <td>-0.035875</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>-0.464734</td>\n",
       "      <td>0.434193</td>\n",
       "      <td>-0.203266</td>\n",
       "      <td>-0.124105</td>\n",
       "      <td>0.153430</td>\n",
       "      <td>0.077496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.935302</td>\n",
       "      <td>-1.948072</td>\n",
       "      <td>1.389767</td>\n",
       "      <td>2.940639</td>\n",
       "      <td>0.546747</td>\n",
       "      <td>-1.226495</td>\n",
       "      <td>-0.936213</td>\n",
       "      <td>0.636376</td>\n",
       "      <td>-0.263805</td>\n",
       "      <td>0.377704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152606</td>\n",
       "      <td>0.133142</td>\n",
       "      <td>-0.018714</td>\n",
       "      <td>-0.461436</td>\n",
       "      <td>0.065495</td>\n",
       "      <td>-0.116545</td>\n",
       "      <td>-0.017650</td>\n",
       "      <td>0.139454</td>\n",
       "      <td>-0.005332</td>\n",
       "      <td>0.003062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>6.439315</td>\n",
       "      <td>-3.576817</td>\n",
       "      <td>2.459487</td>\n",
       "      <td>1.177314</td>\n",
       "      <td>-0.074824</td>\n",
       "      <td>-2.375193</td>\n",
       "      <td>-0.596130</td>\n",
       "      <td>-0.035471</td>\n",
       "      <td>0.987929</td>\n",
       "      <td>0.256989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147642</td>\n",
       "      <td>0.348266</td>\n",
       "      <td>-0.195214</td>\n",
       "      <td>0.246315</td>\n",
       "      <td>-0.404446</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>-0.067000</td>\n",
       "      <td>0.088590</td>\n",
       "      <td>0.107898</td>\n",
       "      <td>0.219520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3.793382</td>\n",
       "      <td>-3.584048</td>\n",
       "      <td>2.088476</td>\n",
       "      <td>-2.506028</td>\n",
       "      <td>-0.510723</td>\n",
       "      <td>-0.246710</td>\n",
       "      <td>-0.716326</td>\n",
       "      <td>-1.113360</td>\n",
       "      <td>-0.105207</td>\n",
       "      <td>-0.108632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138184</td>\n",
       "      <td>0.293495</td>\n",
       "      <td>-0.109046</td>\n",
       "      <td>0.182521</td>\n",
       "      <td>-0.229947</td>\n",
       "      <td>-0.009802</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>-0.055405</td>\n",
       "      <td>0.086135</td>\n",
       "      <td>0.001197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1.256179</td>\n",
       "      <td>-1.902297</td>\n",
       "      <td>0.562731</td>\n",
       "      <td>-2.089227</td>\n",
       "      <td>1.809991</td>\n",
       "      <td>-0.534447</td>\n",
       "      <td>-0.192758</td>\n",
       "      <td>0.341887</td>\n",
       "      <td>0.393917</td>\n",
       "      <td>0.520877</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232648</td>\n",
       "      <td>-0.065615</td>\n",
       "      <td>0.021108</td>\n",
       "      <td>0.042020</td>\n",
       "      <td>-0.081420</td>\n",
       "      <td>-0.036592</td>\n",
       "      <td>0.063352</td>\n",
       "      <td>-0.200312</td>\n",
       "      <td>-0.044819</td>\n",
       "      <td>0.002429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>10.374794</td>\n",
       "      <td>1.672010</td>\n",
       "      <td>-1.877029</td>\n",
       "      <td>-2.356031</td>\n",
       "      <td>-0.033742</td>\n",
       "      <td>0.567936</td>\n",
       "      <td>0.223082</td>\n",
       "      <td>-0.280239</td>\n",
       "      <td>-0.542035</td>\n",
       "      <td>-0.089296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061047</td>\n",
       "      <td>-0.168514</td>\n",
       "      <td>-0.306874</td>\n",
       "      <td>-0.310569</td>\n",
       "      <td>0.173216</td>\n",
       "      <td>0.140648</td>\n",
       "      <td>-0.042478</td>\n",
       "      <td>0.168820</td>\n",
       "      <td>-0.195969</td>\n",
       "      <td>0.377830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-5.475243</td>\n",
       "      <td>-0.670637</td>\n",
       "      <td>1.490443</td>\n",
       "      <td>-2.299157</td>\n",
       "      <td>-0.184703</td>\n",
       "      <td>1.617837</td>\n",
       "      <td>1.698952</td>\n",
       "      <td>1.046354</td>\n",
       "      <td>0.374101</td>\n",
       "      <td>-0.047726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921288</td>\n",
       "      <td>-0.304810</td>\n",
       "      <td>-0.180972</td>\n",
       "      <td>-0.189104</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.274680</td>\n",
       "      <td>-0.243238</td>\n",
       "      <td>0.037915</td>\n",
       "      <td>-0.075111</td>\n",
       "      <td>-0.017508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1         2         3         4         5         6   \\\n",
       "0     9.192837   1.948583 -1.123166  3.633731 -1.195110  1.411424  2.159370   \n",
       "1     2.387802  -3.768172 -0.529293  1.118264  0.621775  0.028656  0.013358   \n",
       "2     5.733896  -1.075174 -0.551748  0.912083 -0.177086  0.541452 -0.668166   \n",
       "3     7.122953  10.275589 -3.232790  0.152547 -2.960878  3.053422  1.429911   \n",
       "4     3.935302  -1.948072  1.389767  2.940639  0.546747 -1.226495 -0.936213   \n",
       "..         ...        ...       ...       ...       ...       ...       ...   \n",
       "564   6.439315  -3.576817  2.459487  1.177314 -0.074824 -2.375193 -0.596130   \n",
       "565   3.793382  -3.584048  2.088476 -2.506028 -0.510723 -0.246710 -0.716326   \n",
       "566   1.256179  -1.902297  0.562731 -2.089227  1.809991 -0.534447 -0.192758   \n",
       "567  10.374794   1.672010 -1.877029 -2.356031 -0.033742  0.567936  0.223082   \n",
       "568  -5.475243  -0.670637  1.490443 -2.299157 -0.184703  1.617837  1.698952   \n",
       "\n",
       "           7         8         9   ...        15        16        17  \\\n",
       "0   -0.398407 -0.157118 -0.877402  ...  0.745116 -0.265471 -0.549563   \n",
       "1    0.240988 -0.711905  1.106995  ... -0.648809 -0.017212  0.318297   \n",
       "2    0.097374  0.024066  0.454275  ...  0.325111  0.190918 -0.087975   \n",
       "3    1.059565 -1.405440 -1.116975  ...  0.051370  0.482634 -0.035875   \n",
       "4    0.636376 -0.263805  0.377704  ... -0.152606  0.133142 -0.018714   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "564 -0.035471  0.987929  0.256989  ...  0.147642  0.348266 -0.195214   \n",
       "565 -1.113360 -0.105207 -0.108632  ... -0.138184  0.293495 -0.109046   \n",
       "566  0.341887  0.393917  0.520877  ... -0.232648 -0.065615  0.021108   \n",
       "567 -0.280239 -0.542035 -0.089296  ... -0.061047 -0.168514 -0.306874   \n",
       "568  1.046354  0.374101 -0.047726  ...  0.921288 -0.304810 -0.180972   \n",
       "\n",
       "           18        19        20        21        22        23        24  \n",
       "0   -0.133768  0.345565  0.096515  0.068850  0.084519 -0.175256 -0.151020  \n",
       "1    0.247565 -0.114133 -0.077327 -0.094578 -0.217718  0.011290 -0.170510  \n",
       "2    0.392626 -0.204532  0.311067 -0.060309 -0.074291  0.102762  0.171158  \n",
       "3    0.026748 -0.464734  0.434193 -0.203266 -0.124105  0.153430  0.077496  \n",
       "4   -0.461436  0.065495 -0.116545 -0.017650  0.139454 -0.005332  0.003062  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "564  0.246315 -0.404446  0.006888 -0.067000  0.088590  0.107898  0.219520  \n",
       "565  0.182521 -0.229947 -0.009802  0.019563 -0.055405  0.086135  0.001197  \n",
       "566  0.042020 -0.081420 -0.036592  0.063352 -0.200312 -0.044819  0.002429  \n",
       "567 -0.310569  0.173216  0.140648 -0.042478  0.168820 -0.195969  0.377830  \n",
       "568 -0.189104  0.163254  0.274680 -0.243238  0.037915 -0.075111 -0.017508  \n",
       "\n",
       "[569 rows x 25 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing Principal Component Analysis  (Number of principal components, K=25)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=25)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalComponents = pd.DataFrame(principalComponents)\n",
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "36ba2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the principal components as the input variables\n",
    "\n",
    "X = principalComponents.iloc[:, [0,1, 2, 3, 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]].values\n",
    "Y = breast_dataset.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b665874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing train_test_split\n",
    "#splitting the dataset into 80% and 20% split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1111d44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-20 {color: black;background-color: white;}#sk-container-id-20 pre{padding: 0;}#sk-container-id-20 div.sk-toggleable {background-color: white;}#sk-container-id-20 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-20 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-20 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-20 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-20 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-20 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-20 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-20 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-20 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-20 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-20 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-20 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-20 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-20 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-20 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-20 div.sk-item {position: relative;z-index: 1;}#sk-container-id-20 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-20 div.sk-item::before, #sk-container-id-20 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-20 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-20 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-20 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-20 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-20 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-20 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-20 div.sk-label-container {text-align: center;}#sk-container-id-20 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-20 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-20\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" checked><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a1661ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the classifier on the test data\n",
    "\n",
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f8615260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n",
      "Precision: 0.9558823529411765\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#importing and printing accuracy, precision and recall of the classifier\n",
    "\n",
    "from sklearn import metrics\n",
    "pos_label=['Benign', 'Malignant']\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred, pos_label= \"Malignant\"))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred, pos_label= \"Malignant\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
